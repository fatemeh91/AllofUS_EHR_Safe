{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, LSTM, RepeatVector, Activation,Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "from scipy import stats\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import models\n",
    "from numpy import array_equal\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#from scipy.ndimage.interpolation import shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_sequence= 72\n",
    "max_output_sequence=72\n",
    "input_dimension=101\n",
    "print('max_input_sequence: ', max_input_sequence)\n",
    "print('max_output_sequence: ', max_output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=np.load('Stochastic_Normalize_data_Seq72.npy')\n",
    "#data=data[:,0:50,0:6]\n",
    "data=pd.read_csv('CostumeStochasticNormalizedData_Seq72_191F.csv')\n",
    "data2=np.asarray(data.values)\n",
    "data2=np.reshape(data2,(10690,72,191))\n",
    "data3=data2[:,:,0:100]\n",
    "data4=data2[:,:,190:191]\n",
    "data2=np.concatenate((data3,data4),axis=2)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[0,0:72,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded =data2\n",
    "y_train_padded=data2\n",
    "print(\"X_train_padded shape: \",X_train_padded.shape)\n",
    "print(\"y_train_padded shape: \",y_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, n_unique):\n",
    "    encoding = list()\n",
    "    for value in sequence:\n",
    "        vector = [0 for _ in range(n_unique)]\n",
    "        vector[value] = 1\n",
    "        encoding.append(vector)\n",
    "    return array(encoding)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_elements(arr, num, fill_value):\n",
    "    result = np.empty_like(arr)\n",
    "    result[:num] = fill_value\n",
    "    result[num:] = arr[:-num]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = X_train_padded.copy()\n",
    "decoder_target_data = y_train_padded.copy()\n",
    "decoder_input_data = decoder_target_data.copy()\n",
    "for i, samples in enumerate(decoder_target_data):\n",
    "    #pdb.set_trace()\n",
    "    shifted=shift_elements(samples,1,-1)\n",
    "    #shifted= shift(samples,[1,0], cval=-1) google implimentation\n",
    "    decoder_input_data[i]=shifted\n",
    "#decoder_target_data=decoder_input_data.copy() # test if it works better\n",
    "print(\"Data for Train\")\n",
    "print('encoder_input_data (X): ', encoder_input_data[1])\n",
    "print('decoder_input_data (teacher forcing): ',decoder_input_data[1])\n",
    "print('decoder_target_data (y):',decoder_target_data[1])\n",
    "print(encoder_input_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMoutputDimension = 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs= Input(shape=(max_input_sequence, input_dimension), name='encoder_inputs')\n",
    "mask_value= [-1]*input_dimension\n",
    "masking = tf.keras.layers.Masking(mask_value=mask_value)\n",
    "encoder_inputs_masked = masking(encoder_inputs)\n",
    "encoder_lstm=LSTM(LSTMoutputDimension, return_state=True, name='encoder_lstm')\n",
    "LSTM_outputs, state_h, state_c = encoder_lstm(encoder_inputs_masked)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = Input(shape=(None, input_dimension), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(LSTMoutputDimension, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(input_dimension, activation='linear', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#mask1=[False]*input_dimension\n",
    "#missing_index=np.arange(1,100,2)\n",
    "#mask1=np.asarray(mask1,dtype=bool)\n",
    "#mask1[missing_index]=True\n",
    "\n",
    "#mask2=~mask1\n",
    "#decoder_softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "#soft_decoder_outputs=decoder_softmax(decoder_outputs, mask1)\n",
    "\n",
    "#new_decoder_outputs = tf.keras.layers.Add()([decoder_outputs,soft_decoder_outputs])\n",
    "\n",
    "model_encoder_training = Model([encoder_inputs, decoder_inputs],decoder_outputs, name='model_encoder_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.losses.MeanSquaredError()\n",
    "#loss_object = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "#loss_BCE=tf.keras.losses.BinaryCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)\n",
    "#loss= loss_object+loss_BCE\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    loss=0\n",
    "    loss_object = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    #loss_BCE=tf.keras.losses.BinaryCrossentropy(from_logits=False,reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    \n",
    "    #missing_index=[1]#np.arange(1,100,2)\n",
    "    #all_index=[i for i in range(3) if not i in missing_index]\n",
    "\n",
    "    loss=loss_object(y_true,y_pred)\n",
    "    \n",
    "    #loss1=loss_object(tf.gather(y_true,all_index),tf.gather(y_pred,all_index))\n",
    "    #loss2=loss_BCE(tf.gather(y_true,missing_index),tf.gather(y_pred,missing_index))\n",
    "    #loss=tf.reduce_mean(loss1)+tf.reduce_mean(loss2) \n",
    "    return loss\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model_encoder_training.compile(optimizer=opt, loss=custom_loss, metrics=['accuracy'])\n",
    "model_encoder_training.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_encoder_training, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-6, mode='min', verbose=1, patience=500,restore_best_weights=True)\n",
    "model_encoder_training.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=1000,\n",
    "          epochs=5000,\n",
    "          validation_split=0.1,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def train_test(model, X_train, y_train , X_test, y_test, epochs=500, batch_size=500, patience=5,verbose=0):\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "    print('training for ',epochs,' epochs begins with EarlyStopping(monitor= val_loss, patience=',patience,')....')\n",
    "    history=model.fit(X_train, y_train, validation_split= 0.1, epochs=epochs,batch_size=batch_size, verbose=verbose, callbacks=[es])\n",
    "    print(epochs,' epoch training finished...')\n",
    "    _, train_acc = model.evaluate(X_train, y_train, batch_size=batch_size, verbose=0)\n",
    "    _, test_acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print('\\nPREDICTION ACCURACY (%):')\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc*100, test_acc*100))\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(model.name+' accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(model.name+' loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "train_test(model_encoder_training, [encoder_input_data, decoder_input_data], decoder_target_data ,\n",
    "           [encoder_input_data, decoder_input_data], \n",
    "           decoder_target_data, epochs=20, batch_size=200, patience=2,verbose=2)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(LSTMoutputDimension,))\n",
    "decoder_state_input_c = Input(shape=(LSTMoutputDimension,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(states_value):\n",
    "    \n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, input_dimension))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0,:] = -1 \n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_seq = list()\n",
    "    while not stop_condition:\n",
    "\n",
    "        # in a loop\n",
    "        # decode the input to a token/output prediction + required states for context vector\n",
    "        #pdb.set_trace()\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # add the predicted token/output to output sequence\n",
    "        decoded_seq.append(output_tokens[0, -1, :])\n",
    "        \n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        \n",
    "      \n",
    "        if (np.sum(output_tokens[0, -1, :]==[-1]*3)>1 or\n",
    "           len(decoded_seq) == max_output_sequence or \n",
    "           -1 in output_tokens[0, -1,:]):\n",
    "            stop_condition = True\n",
    "\n",
    "        # with the predicted token/output \n",
    "        target_seq = output_tokens\n",
    "        # updating\n",
    "        states_value = [h, c]\n",
    "     \n",
    "    decoded_seq=np.asarray(decoded_seq)\n",
    "    decoded_seq=np.reshape(decoded_seq,(len(decoded_seq),input_dimension))\n",
    "    return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct =0 \n",
    "sampleNo =  1\n",
    "for sample in range(0,sampleNo):\n",
    "    # Encode the input as state vectors\n",
    "    pdb.set_trace()\n",
    "    states_value = encoder_model.predict(encoder_input_data[sample].reshape(1,max_input_sequence,input_dimension))\n",
    "    predicted= decode_sequence(states_value)\n",
    "    if (decoder_target_data[sample])== predicted+ [-1] * (max_output_sequence- len(predicted)):\n",
    "        correct+=1\n",
    "print('Accuracy: ', correct/sampleNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.abs(decoder_target_data[sample]-predicted)\n",
    "input_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(1+math.e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.tanh(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.tanh(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
